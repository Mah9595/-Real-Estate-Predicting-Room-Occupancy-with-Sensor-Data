{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In Part III, we will use machine learning techniques to predict 'Occupancy'. The process goes like this: \n",
    "\n",
    "![MachineLearningProcess](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/CommonAssets/MachineLearningProcess.png)\n",
    "\n",
    "We put this section on all of the projects in UpLevel so bear with us if you've seen this before. \n",
    "\n",
    "Generally, the machine learning process has five parts:\n",
    "1. <strong>Split your data into train and test set</strong>\n",
    "2. <strong>Model creation</strong>\n",
    "<br>\n",
    "Import your models from sklearn and instantiate them (assign model object to a variable)\n",
    "3. <strong>model fitting</strong>\n",
    "<br>\n",
    "Fit your training data into the model and train train train\n",
    "4. <strong>model prediction</strong>\n",
    "<br>\n",
    "Make a set of predictions using your test data, and\n",
    "5. <strong>Model assessment</strong>\n",
    "<br>\n",
    "Compare your predictions with ground truth in test data\n",
    "\n",
    "Highly recommended readings:\n",
    "1. [Important] https://scipy-lectures.org/packages/scikit-learn/index.html\n",
    "2. https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/\n",
    "3. https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "### Step 1: Import your libraries\n",
    "We will be using models from sklearn - a popular machine learning library. However, we won't import everything from sklearn and take just what we need. \n",
    "\n",
    "We'll need to import plotting libraries to plot our predictions against the ground truth (test data). \n",
    "\n",
    "Import the following:\n",
    "- pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import your library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the CSV from Part II as a DataFrame\n",
    "Read your CSV from the previous Part as a DataFrame. \n",
    "\n",
    "You should have:\n",
    "- 20,560 rows\n",
    "- 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20560 entries, 0 to 20559\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           20560 non-null  object \n",
      " 1   Temperature    20560 non-null  float64\n",
      " 2   Humidity       20560 non-null  float64\n",
      " 3   Light          20560 non-null  float64\n",
      " 4   CO2            20560 non-null  float64\n",
      " 5   HumidityRatio  20560 non-null  float64\n",
      " 6   Occupancy      20560 non-null  int64  \n",
      " 7   weekday        20560 non-null  int64  \n",
      " 8   hour           20560 non-null  int64  \n",
      " 9   minute         20560 non-null  int64  \n",
      "dtypes: float64(5), int64(4), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Read the CSV from Part II\n",
    "df = pd.read_csv('/Users/sm/Desktop/Ex2.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare your independent and dependent variables\n",
    "At this point, let's prepare our indepedent and dependent variables. \n",
    "\n",
    "1. Declare a variable, and assign your independent variables to it by dropping 'date' and 'Occupancy'\n",
    "2. Declare another variable, and assign only values 'Occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare your independent and dependent variables\n",
    "X = df.drop(['date','Occupancy'],axis=1)\n",
    "y = df['Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import machine learning libraries\n",
    "Time to import other libraries.\n",
    "\n",
    "The resources provided at the top of this notebook will be immensely useful if you're new to modelling. \n",
    "\n",
    "Import the following libraries and methods:\n",
    "1. train_test_split - sklearn.model_selection\n",
    "2. DummyClassifier - sklearn.dummy\n",
    "3. LogisticRegression - sklearn.linear_model\n",
    "4. DecisionTreeClassifier - sklearn.tree\n",
    "5. RandomForestClassifier - sklearn.ensemble\n",
    "6. f1_score - sklearn.metrics\n",
    "7. confusion_matrix - sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Import the machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Split your dataset into train and test\n",
    "Now that you have finished importing the libraries you need, split the dataset into train and test at a 80/20 split.\n",
    "\n",
    "Don't forget to stratify by your dependent values with the stratify parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 4750, 0: 15810})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30044275774826057"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4750/15810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split your dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({0: 11857, 1: 3563})\n",
      "Test: Counter({0: 3953, 1: 1187})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", Counter(y_train))\n",
    "print(\"Test:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train a DummyClassifier\n",
    "This is what you'll need to do:\n",
    "1. Start with a model\n",
    "2. Declare a variable, and store your model in it (don't forget to use brackets)\n",
    "3. Fit your training data into the instantiated model\n",
    "4. Declare a variable that contains predictions from the model you just trained, using the train dataset (X_test)\n",
    "5. Compare the prediction with the actual result (y_test) with the f1_score\n",
    "6. Plot a confusion_matrix using the prediction (y-axis) vs actual y_test (x-axis) \n",
    "\n",
    "The recommended readings will be very helpful.\n",
    "\n",
    "Let's start with the DummyClassifier to establish a baseline. This will be useful as we train other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.4347300120972177\n",
      "[[3953    0]\n",
      " [1187    0]]\n"
     ]
    }
   ],
   "source": [
    "# Step 6a: Declare a variable to store the model\n",
    "dummy_clf = DummyClassifier()\n",
    "# Step 6b: Fit your train dataset\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "# Step 6c: Declare a variable and store your predictions that you make with your model using X test data\n",
    "pred_dummy = dummy_clf.predict(X_test)\n",
    "# Step 6d: Print the f1_score between the y test and dummy prediction\n",
    "print(\"f1-score: \", f1_score(y_test, pred_dummy, average='macro'))\n",
    "# Step 6e: Print a confusion_matrix between y_test and your prediction\n",
    "print(confusion_matrix(y_test, pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train a LogisticRegression\n",
    "Now that we have established the baseline performance of a classifier, let's train a LogisticRegression model. \n",
    "\n",
    "Similar to how we did in training the DummyClassifier, train the model and then assess the model performance with the f1_score and the confusion_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.9856493188062142\n",
      "[[3907   46]\n",
      " [   7 1180]]\n"
     ]
    }
   ],
   "source": [
    "# Step 7a: Declare a variable to store the LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "# Step 7b: Fit your train dataset\n",
    "clf.fit(X_train, y_train)\n",
    "# Step 7c: Declare a variable and store your predictions that you make with your model using X test data \n",
    "pred_LR = clf.predict(X_test)\n",
    "# Step 7d: Print f1_score between the y test and LogisticRegression prediction\n",
    "print(\"f1-score: \", f1_score(y_test, pred_LR, average='macro'))\n",
    "# Step 7e: Print a confusion_matrix between y_test and your prediction\n",
    "print(confusion_matrix(y_test, pred_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train a DecisionTreeClassifier\n",
    "The LogisticRegression model should perform quite impressively, based on the confusion matrix and the f1_score. \n",
    "\n",
    "Can we improve it further? Let's find out by training and assessing a DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.9887285277624855\n",
      "[[3939   14]\n",
      " [  27 1160]]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train a DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(X_train, y_train)\n",
    "pred_DT = clf1.predict(X_test)\n",
    "print(\"f1-score: \", f1_score(y_test, pred_DT, average='macro'))\n",
    "print(confusion_matrix(y_test, pred_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Train a RandomForestClassifier\n",
    "The DecisionTreeClassifier is most likely (slightly) better than the LogisticRegression results, in terms of f1 score.\n",
    "\n",
    "Train a RandomForestClassifier and see if you can push the performance even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.9915078953815775\n",
      "[[3938   15]\n",
      " [  16 1171]]\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Train a RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier()\n",
    "clf2.fit(X_train, y_train)\n",
    "pred_RF = clf2.predict(X_test)\n",
    "print(\"f1-score: \", f1_score(y_test, pred_RF, average='macro'))\n",
    "print(confusion_matrix(y_test, pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Train other classifiers\n",
    "There are a few other classifiers that you can try, apart from the three that we used above.\n",
    "\n",
    "It's hard to top RandomForestClassifier for this dataset, but it's still worth typing it out to get some practice in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Get a feature importances DataFrame\n",
    "Create a DataFrame containing the feature importances of your best performing model. \n",
    "\n",
    "For example, this is what an example DataFrame would look like:\n",
    "\n",
    "![RandomForestClassifierFeatureImportances](https://uplevelsg.s3.ap-southeast-1.amazonaws.com/ProjectRoomOccupancy/RandomForestClassifierFeatureImportances.png)\n",
    "\n",
    "What's the most important feature? \n",
    "\n",
    "Does it align with what you observed in Part II? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>0.178135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.019396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Light</td>\n",
       "      <td>0.533323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CO2</td>\n",
       "      <td>0.097639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HumidityRatio</td>\n",
       "      <td>0.028111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weekday</td>\n",
       "      <td>0.048848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hour</td>\n",
       "      <td>0.085892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minute</td>\n",
       "      <td>0.008656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  importance\n",
       "0    Temperature    0.178135\n",
       "1       Humidity    0.019396\n",
       "2          Light    0.533323\n",
       "3            CO2    0.097639\n",
       "4  HumidityRatio    0.028111\n",
       "5        weekday    0.048848\n",
       "6           hour    0.085892\n",
       "7         minute    0.008656"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 10: Create a DataFrame containing feature importances\n",
    "pd.DataFrame({'feature':X_train.columns, 'importance':clf2.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling without 'Light'\n",
    "Whichever model you used, it's most likely that you identified \"Light\" as the most important feature in the model.\n",
    "\n",
    "This makes sense, because if there's 'Light', it's most likely that there's someone in the room. \n",
    "\n",
    "Here's a challenge - let's try modelling without \"Light\" as a feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Repeat Step 3 and drop 'Light'\n",
    "Repeat what you did in Step 3, i.e. prepare independent and dependent values.\n",
    "\n",
    "However, this time drop 'Light' on top of 'date' and 'Occupancy' to prepare your independent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Prepare new independent and dependent values\n",
    "X1 = df.drop(['date','Occupancy','Light'],axis=1)\n",
    "y1 = df['Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Repeat Steps 6-10\n",
    "Now that you've removed 'Light' column, time to split your data and model again.\n",
    "\n",
    "One thing to note - when you train a LogisticRegression you <strong>may</strong> receive a warning. Don't worry - just increase the value of max_iter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12a: Split your data into train and test\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.25, random_state=0, stratify=y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.4347300120972177\n",
      "[[3953    0]\n",
      " [1187    0]]\n"
     ]
    }
   ],
   "source": [
    "# Step 12b: Train and assess a DummyClassifier\n",
    "dummy_clf1 = DummyClassifier()\n",
    "dummy_clf1.fit(X_train1, y_train1)\n",
    "pred_dummy1 = dummy_clf1.predict(X_test1)\n",
    "print(\"f1-score: \", f1_score(y_test1, pred_dummy1, average='macro'))\n",
    "print(confusion_matrix(y_test1, pred_dummy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.7826083338655963\n",
      "[[3701  252]\n",
      " [ 485  702]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sm/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Step 12b: Train and assess a LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_a = LogisticRegression()\n",
    "clf_a.fit(X_train1, y_train1)\n",
    "pred_LR1 = clf_a.predict(X_test1)\n",
    "print(\"f1-score: \", f1_score(y_test1, pred_LR1, average='macro'))\n",
    "print(confusion_matrix(y_test1, pred_LR1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.9832501512502687\n",
      "[[3927   26]\n",
      " [  35 1152]]\n"
     ]
    }
   ],
   "source": [
    "# Step 12c: Train a DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf1_a = DecisionTreeClassifier()\n",
    "clf1_a.fit(X_train1, y_train1)\n",
    "pred_DT1 = clf1_a.predict(X_test1)\n",
    "print(\"f1-score: \", f1_score(y_test1, pred_DT1, average='macro'))\n",
    "print(confusion_matrix(y_test1, pred_DT1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:  0.9882136741111288\n",
      "[[3933   20]\n",
      " [  23 1164]]\n"
     ]
    }
   ],
   "source": [
    "# Step 12d: Train a RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2_a = RandomForestClassifier()\n",
    "clf2_a.fit(X_train1, y_train1)\n",
    "pred_RF1 = clf2_a.predict(X_test1)\n",
    "print(\"f1-score: \", f1_score(y_test1, pred_RF1, average='macro'))\n",
    "print(confusion_matrix(y_test1, pred_RF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>0.279353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.063016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO2</td>\n",
       "      <td>0.211105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HumidityRatio</td>\n",
       "      <td>0.074404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekday</td>\n",
       "      <td>0.105689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hour</td>\n",
       "      <td>0.241689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>minute</td>\n",
       "      <td>0.024744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  importance\n",
       "0    Temperature    0.279353\n",
       "1       Humidity    0.063016\n",
       "2            CO2    0.211105\n",
       "3  HumidityRatio    0.074404\n",
       "4        weekday    0.105689\n",
       "5           hour    0.241689\n",
       "6         minute    0.024744"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12e: Create a DataFrame containing feature importances\n",
    "pd.DataFrame({'feature':X_train1.columns, 'importance':clf2_a.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Did removing 'Light' affect model performance adversely?</strong></summary>\n",
    "    <div>No, not really. The f1 score and confusion matrix look great</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>What were the features that were important?</strong></summary>\n",
    "    <div>In the new DataFrame, the features were 'Temperature', 'CO2', and 'hour'. It seems that the model considered these three features in the absence of light.</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end\n",
    "You did it! You've arrived at the end. Congratulations and well done on completing this project series! \n",
    "\n",
    "Let's review.\n",
    "1. In Part I, you collected the datasets and combined them to form a single DataFrame. You also investigated the data briefly to see if there was anything remarkable about it\n",
    "2. In Part II, you performed exploratory data analysis on the dataset, investigating distributions and relationships found between features. You also engineered additional features from the dataset for model building\n",
    "5. In Part III, you trained a machine learning model that can predict room occupancy based on sensor data. In addition, you modelled the problem without a major feature to see if the model performed equally well\n",
    "\n",
    "Go on, give yourself a pat on the back. We hope this project series has give you more confidence in coding and machine learning. \n",
    "\n",
    "Whatever you learn here is but a tip of the iceberg, and launchpad for bigger and better things to come. Come join us in our Telegram community over at https://bit.ly/UpLevelSG and our Facebook page at https://fb.com/UpLevelSG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
